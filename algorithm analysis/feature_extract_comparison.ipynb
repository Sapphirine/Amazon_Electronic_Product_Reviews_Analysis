{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from operator import attrgetter\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import chi2,SelectKBest\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from operator import attrgetter\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure\n",
    "import nltk\n",
    "\n",
    "def cleantext( raw_data):\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", raw_data) \n",
    "    words = word_tokenize(letters_only.lower())                         \n",
    "    meaningful_words = [ WordNetLemmatizer().lemmatize(w,'v') for w in words if not w in stops]\n",
    "    meaningful_words = [w for w in meaningful_words if (len(w) >2) and (not w in white) ]\n",
    "    return (' ').join(meaningful_words)\n",
    "\n",
    "def feature_select(x):\n",
    "    if ' 'in x[0]:\n",
    "        temp=nltk.pos_tag(nltk.word_tokenize(x[0]))\n",
    "        temp=zip(*temp)\n",
    "        term=[('JJ','NN'),('NN','JJ'),('RB','VB'),('RB','NN'),('RB','JJ'),('NN','NN'),('RB','RB')]\n",
    "        return( temp[1] in term)\n",
    "\n",
    "def as_matrix(vec):\n",
    "    data, indices = vec.values, vec.indices\n",
    "    shape = 1, vec.size\n",
    "    return csr_matrix((data, indices, np.array([0, vec.values.size])), shape)\n",
    "\n",
    "\n",
    "productlist=pd.read_excel('productinfo.xlsx')\n",
    "bwlist=pd.read_excel('whitelist.xlsx')\n",
    "white=bwlist['white'].tolist()\n",
    "black=set(bwlist['black'].tolist())\n",
    "productname=pd.read_excel('productname.xlsx')\n",
    "stemmer = PorterStemmer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops=stops|black\n",
    "\n",
    "amazon = sqlContext.read.json(\"Electronics_5.json\")\n",
    "amazon1=amazon.withColumn(\"output\",(amazon['overall']>3).cast('int').cast('int'))\n",
    "amazon1=amazon1.select('*',concat(col(\"summary\"), lit(\" \"), col(\"reviewText\")).alias(\"reviewsum\"))\n",
    "#amazon1=amazon1.filter(amazon1.output>-1)\n",
    "amazon1=amazon1.drop('reviewText')\n",
    "clean = udf(lambda s: cleantext(s), StringType())\n",
    "amazon1= amazon1.withColumn('reviewsum', clean(amazon1['reviewsum']))\n",
    "\n",
    "\n",
    "\n",
    "gram=list()\n",
    "onew=list()\n",
    "tword=list()\n",
    "ti1=list()\n",
    "ti2=list()\n",
    "ti3=list()\n",
    "cou=list()\n",
    "import timeit\n",
    "from itertools import combinations\n",
    "from sklearn.feature_selection import chi2,SelectKBest\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.ml.feature import CountVectorizer # Import the stop word list\n",
    "#from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from operator import attrgetter\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure\n",
    "import nltk\n",
    "\n",
    "for x in productlist['asin'][::-1][-20:]:\n",
    "    print(x)\n",
    "    amazontemp=amazon1.filter(amazon1.asin==x)\n",
    "    start = timeit.default_timer()\n",
    "    tokenizer = Tokenizer(inputCol=\"reviewsum\", outputCol=\"words\")\n",
    "    rating_time = tokenizer.transform(amazontemp)\n",
    "    \n",
    "    cv = CountVectorizer(inputCol=\"words\", outputCol=\"vectors\", minDF=3.0)\n",
    "    model = cv.fit(rating_time)\n",
    "    #result = model.transform(rating_time)\n",
    "    vocab_term=model.vocabulary\n",
    "    onew.append(len(vocab_term))\n",
    "    \n",
    "    \n",
    "    stop1 = timeit.default_timer()\n",
    "    \n",
    "    from pyspark.ml.feature import NGram\n",
    "    ngram = NGram(n=2,inputCol=\"words\", outputCol=\"ngrams\")\n",
    "    ngramDataFrame = ngram.transform(rating_time)\n",
    "    rating_time=ngramDataFrame\n",
    "    \n",
    "    #form term frequency table\n",
    "    #cv = CountVectorizer(inputCol=\"ngrams\", outputCol=\"vectors\", minDF=3.0)\n",
    "    #model = cv.fit(rating_time)\n",
    "    #result = model.transform(rating_time)\n",
    "    #vocab_term=model.vocabulary\n",
    "    #gram.append(len(vocab_term))\n",
    "    \n",
    "    twcom=[]\n",
    "    def twoword(doc):\n",
    "        term=doc.split()\n",
    "        temp=[]\n",
    "        #temp += [\"vvv\".join(a) for a in combinations(term[0:(0+4)], 2)]\n",
    "        for i in (range(1,len(term)-4)):\n",
    "            for k in range(0,3):\n",
    "                temp += [term[i+k]+'vvv'+term[i+k+1]]\n",
    "        return (\" \".join(temp))\n",
    "\n",
    "    tw = udf(lambda s: twoword(s), StringType())\n",
    "    rating_time=rating_time.withColumn('reviewsum1', tw(ngramDataFrame['reviewsum']))\n",
    "    from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "    tokenizer = Tokenizer(inputCol=\"reviewsum1\", outputCol=\"words1\")\n",
    "    wordsData = tokenizer.transform(rating_time)\n",
    "    cv = CountVectorizer(inputCol=\"words1\", outputCol=\"vectors\", minDF=3.0)\n",
    "    model = cv.fit(wordsData)\n",
    "    #result = model.transform(wordsData)\n",
    "    vocab_term=model.vocabulary\n",
    "    tword.append(len(vocab_term))\n",
    "    \n",
    "    stop2 = timeit.default_timer()\n",
    "    \n",
    "    twcom=[]\n",
    "    def twoword(doc):\n",
    "        term=doc.split()\n",
    "        temp=[]\n",
    "        temp += [\"vvv\".join(a) for a in combinations(term[0:(0+4)], 2)]\n",
    "        for i in (range(1,len(term)-4)):\n",
    "            for k in range(0,3):\n",
    "                temp += [term[i+k]+'vvv'+term[i+4]]\n",
    "        return (\" \".join(temp))\n",
    "\n",
    "    tw = udf(lambda s: twoword(s), StringType())\n",
    "    rating_time=rating_time.withColumn('reviewsum1', tw(ngramDataFrame['reviewsum']))\n",
    "    from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "    tokenizer = Tokenizer(inputCol=\"reviewsum1\", outputCol=\"words1\")\n",
    "    wordsData = tokenizer.transform(rating_time)\n",
    "    cv = CountVectorizer(inputCol=\"words1\", outputCol=\"vectors\", minDF=3.0)\n",
    "    model = cv.fit(wordsData)\n",
    "    #result = model.transform(wordsData)\n",
    "    vocab_term=model.vocabulary\n",
    "    tword.append(len(vocab_term))\n",
    "    \n",
    "    stop3 = timeit.default_timer()\n",
    "    \n",
    "    ti1.append(stop1-start)\n",
    "    ti2.append(stop2-start)\n",
    "    ti3.append(stop3-stop2)\n",
    "\n",
    "cou=productlist['count(summary)'][::-1][-20:]\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "line1, = plt.plot(cou,ti1, marker='o', label='single word')\n",
    "line2, = plt.plot(cou,ti2, marker='o', label='2 grams')\n",
    "line3, = plt.plot(cou,ti3, marker='o', label='2 words combination')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "ti22=list()\n",
    "for x in range(20):\n",
    "    ti22.append(ti2[x]-ti1[x])\n",
    "\n",
    "cou=productlist['count(summary)'][::-1][-20:]\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(cou,ti1, marker='o', label='single word')\n",
    "line2, = plt.plot(cou,ti22, marker='o', label='2 grams')\n",
    "line3, = plt.plot(cou,ti3, marker='o', label='2 words combination')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "cou=productlist['count(summary)'][::-1][-20:]\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "line1, = plt.plot(cou,onew, marker='o', label='single word')\n",
    "line2, = plt.plot(cou,gram, marker='o', label='2 grams')\n",
    "line3, = plt.plot(cou,tword, marker='o', label='2 words combination')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
