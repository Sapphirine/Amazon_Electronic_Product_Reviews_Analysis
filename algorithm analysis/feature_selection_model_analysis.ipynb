{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import combinations\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from operator import attrgetter\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import chi2,SelectKBest\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from operator import attrgetter\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure\n",
    "import nltk\n",
    "\n",
    "def cleantext( raw_data):\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", raw_data) \n",
    "    words = word_tokenize(letters_only.lower())                         \n",
    "    meaningful_words = [ WordNetLemmatizer().lemmatize(w,'v') for w in words if not w in stops]\n",
    "    meaningful_words = [w for w in meaningful_words if (len(w) >2) and (not w in white) ]\n",
    "    return (' ').join(meaningful_words)\n",
    "\n",
    "def feature_select(x):\n",
    "    if ' 'in x[0]:\n",
    "        temp=nltk.pos_tag(nltk.word_tokenize(x[0]))\n",
    "        temp=zip(*temp)\n",
    "        term=[('JJ','NN'),('NN','JJ'),('RB','VB'),('RB','NN'),('RB','JJ'),('NN','NN'),('RB','RB')]\n",
    "        return( temp[1] in term)\n",
    "\n",
    "def as_matrix(vec):\n",
    "    data, indices = vec.values, vec.indices\n",
    "    shape = 1, vec.size\n",
    "    return csr_matrix((data, indices, np.array([0, vec.values.size])), shape)\n",
    "\n",
    "\n",
    "productlist=pd.read_excel('productinfo.xlsx')\n",
    "bwlist=pd.read_excel('whitelist.xlsx')\n",
    "white=set(bwlist['white'].tolist())\n",
    "black=set(bwlist['black'].tolist())\n",
    "productname=pd.read_excel('productname.xlsx')\n",
    "stemmer = PorterStemmer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops=stops|black\n",
    "\n",
    "amazon = sqlContext.read.json(\"Electronics_5.json\")\n",
    "amazon1=amazon.withColumn(\"output\",(amazon['overall']>3).cast('int').cast('int'))\n",
    "amazon1=amazon1.select('*',concat(col(\"summary\"), lit(\" \"), col(\"reviewText\")).alias(\"reviewsum\"))\n",
    "#amazon1=amazon1.filter(amazon1.output>-1)\n",
    "amazon1=amazon1.drop('reviewText')\n",
    "clean = udf(lambda s: cleantext(s), StringType())\n",
    "amazon1= amazon1.withColumn('reviewsum', clean(amazon1['reviewsum']))\n",
    "twcom=[]\n",
    "def twoword(doc):\n",
    "    term=doc.split()\n",
    "    temp=[]\n",
    "    #temp += [\"vvv\".join(a) for a in combinations(term[0:(0+4)], 2)]\n",
    "    for i in (range(len(term)-1)):\n",
    "            temp += [term[i]+'vvv'+term[i+1]]\n",
    "    return (\" \".join(temp))\n",
    "\n",
    "tw = udf(lambda s: twoword(s), StringType())\n",
    "amazon1=amazon1.withColumn('reviewsum1', tw(amazon1['reviewsum']))\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"reviewsum1\", outputCol=\"ngrams\")\n",
    "wordsData = tokenizer.transform(amazon1)\n",
    "rating_time=wordsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rating_time=wordsData\n",
    "corr=list()\n",
    "for x in range(1):\n",
    "    x=\"B007WTAJTO\"\n",
    "    rating_time=rating_time.filter(rating_time.asin==x)\n",
    "    #form term frequency table\n",
    "    cv = CountVectorizer(inputCol=\"ngrams\", outputCol=\"vectors\", minDF=3.0)\n",
    "    model = cv.fit(rating_time)\n",
    "    result = model.transform(rating_time)\n",
    "    features = result.rdd.map(attrgetter(\"vectors\"))\n",
    "    def as_matrix(vec):\n",
    "        data, indices = vec.values, vec.indices\n",
    "        shape = 1, vec.size\n",
    "        from scipy.sparse import csr_matrix\n",
    "        return csr_matrix((data, indices, np.array([0, vec.values.size])), shape)\n",
    "\n",
    "    mats = features.map(as_matrix)\n",
    "    mat = vstack(mats.collect()).toarray()\n",
    "    output=result.select('output').rdd.flatMap(lambda x: x).collect()\n",
    "    output=np.array(output)\n",
    "    vocab_term=model.vocabulary\n",
    "    vocab_term=[x.replace('vvv',' ') for x in vocab_term]\n",
    "   \n",
    "\n",
    "    ################################CHI FUNC START####################################################\n",
    "    def feature_select(x):\n",
    "        from nltk import pos_tag\n",
    "        temp=pos_tag(nltk.word_tokenize(x[0]))\n",
    "        temp=zip(*temp)\n",
    "        term=[('JJ','NN'),('RB','NN'),('NN','JJ'),('RB','VB'),('RB','NN'),('RB','JJ'),('RB','RB')]\n",
    "        return( temp[1] in term)\n",
    "\n",
    "    chi2score = chi2(mat,output)\n",
    "    wscores = zip(vocab_term,chi2score[0],chi2score[1])\n",
    "    wchi2 = sorted(wscores,key=lambda x:x[1]) \n",
    "    chitemp=[x for x in wchi2 if feature_select(x)]\n",
    "    topchi2 = zip(*chitemp[-25:])\n",
    "    x = range(len(topchi2[1]))\n",
    "    chilabels = zip(*chitemp[-50:])[0]\n",
    "    x = range(len(topchi2[1]))\n",
    "    labels = topchi2[0]\n",
    "    barh(x,topchi2[1],align='center',alpha=.2,color='g')\n",
    "    plot(topchi2[1],x,'-o',markersize=2,alpha=.8,color='g')\n",
    "    yticks(x,labels)\n",
    "    xlabel('$\\chi^2$')\n",
    "    show()\n",
    "    #################################CHI FUNC END######################################################\n",
    "    #############################BNS FUNC START####################################################\n",
    "    #bi-normal separation feature selection\n",
    "    #vocab_term=[x.replace('vvv',' ') for x in vocab_term]\n",
    "    def bns_select(x,y):\n",
    "        import numpy as np\n",
    "        np.seterr(divide='ignore', invalid='ignore')\n",
    "        from scipy.stats import norm\n",
    "        z=(x>0)*1\n",
    "        tpr=np.dot(y,z) / y.sum()\n",
    "        fpr=np.dot(1-y,z) / (1-y).sum()\n",
    "        tpr[tpr==0]=0.00001\n",
    "        fpr[fpr==0]=0.00001\n",
    "        score=norm.ppf(tpr)-norm.ppf(fpr)\n",
    "        return score\n",
    "\n",
    "    bnsscore = zip(vocab_term,bns_select(mat,output))\n",
    "    wbns = sorted(bnsscore,key=lambda x:float(x[1])) \n",
    "    wbnstemp=[x for x in wbns if feature_select(x)]\n",
    "    toppos = zip(*wbnstemp[-25:])\n",
    "    topneg = zip(*wbnstemp[:25])\n",
    "    x = range(len(toppos[1]))\n",
    "    labels = toppos[0]\n",
    "    barh(x,toppos[1],align='center',alpha=.2,color='g')\n",
    "    plot(toppos[1],x,'-o',markersize=2,alpha=.8,color='g')\n",
    "    yticks(x,labels)\n",
    "    xlabel('bns positive feature')\n",
    "    show()\n",
    "    toppos=topneg\n",
    "    x = range(len(toppos[1]))\n",
    "    labels = toppos[0]\n",
    "    barh(x,toppos[1],align='center',alpha=.2,color='g')\n",
    "    plot(toppos[1],x,'-o',markersize=2,alpha=.8,color='g')\n",
    "    yticks(x,labels)\n",
    "    xlabel('bns negtive feature')\n",
    "    show()\n",
    "    for x in zip(*chitemp[-50:])[0]:\n",
    "        if x in toppos[0]+topneg[0]:\n",
    "            a+=1\n",
    "            print(x)\n",
    "    corr.append(a/len(topchi2[0]))\n",
    "\n",
    "    chilabels = zip(*chitemp[-500:])[0]\n",
    "    f=[ (x in chilabels) for x in vocab_term]\n",
    "    outcome=result.select('output').rdd.flatMap(lambda x: x).collect()\n",
    "    #timeindex=result.select('time').rdd.flatMap(lambda x: x).collect()\n",
    "    #outcome=[4-x if x <4 else 0 for x in output]\n",
    "    train_data_features_term=np.transpose(np.transpose(mat)[f])\n",
    "    from nltk.classify import MaxentClassifier\n",
    "    def word_feats(doc,wlist):\n",
    "     return dict([(wlist[x], True) for x in range(len(wlist)) if doc[x]>0])\n",
    "\n",
    "    trainfeats=[(word_feats(train_data_features_term[x],chilabels),outcome[x]) \n",
    "                for x in range(len(outcome))]\n",
    "    classifier = MaxentClassifier.train(trainfeats,max_iter=10)\n",
    "    classifier.show_most_informative_features(n=50, show='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
